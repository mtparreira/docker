{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ac29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import inflect\n",
    "import unicodedata\n",
    "import contractions\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk        import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem   import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "from pymongo     import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac0b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalidas(frase):\n",
    "    validas =[]\n",
    "    nova_frase = \"\"\n",
    "    palavras = frase.split(\" \")\n",
    "    for palavra in palavras:\n",
    "        if ((len(palavra) > 1) and (len(palavra) < 16)):\n",
    "            if (palavra[0] != '@'):\n",
    "                if (palavra.lower() != \"rt\"):\n",
    "                    validas.append(palavra)\n",
    "    for valida in validas:\n",
    "        nova_frase = nova_frase + valida + \" \"\n",
    "    return nova_frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe22cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nao_ascii(palavras):\n",
    "    novas_palavras = []\n",
    "    for palavra in palavras:\n",
    "        nova_palavra = unicodedata.normalize('NFKD', palavra).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        novas_palavras.append(nova_palavra)\n",
    "    return novas_palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a796a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caixa_baixa(palavras):\n",
    "    novas_palavras = []\n",
    "    for palavra in palavras:\n",
    "        nova_palavra = palavra.lower()\n",
    "        novas_palavras.append(nova_palavra)\n",
    "    return novas_palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c18ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pontuacao(palavras):\n",
    "    novas_palavras = []\n",
    "    for palavra in palavras:\n",
    "        nova_palavra = re.sub(r'[^\\w\\s]', '', palavra)\n",
    "        if nova_palavra != '':\n",
    "            novas_palavras.append(nova_palavra)\n",
    "    return novas_palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d43004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def troca_numero_palavra(palavras):\n",
    "    novas_palavras = []\n",
    "    ie = inflect.engine()\n",
    "    for palavra in palavras:\n",
    "        if palavra.isdigit():\n",
    "            nova_palavra = ie.number_to_words(palavra)\n",
    "            novas_palavras.append(nova_palavra)\n",
    "        else:\n",
    "            novas_palavras.append(palavra)\n",
    "    return novas_palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c534a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(palavras):\n",
    "    novas_palavras = []\n",
    "    for palavra in palavras:\n",
    "        if palavra not in stopwords.words('english'):\n",
    "            novas_palavras.append(palavra)\n",
    "    return novas_palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e1de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratar_troncos(palavras):    \n",
    "    stems = []\n",
    "    stemmer = LancasterStemmer()\n",
    "    for palavra in palavras:\n",
    "        stem = stemmer.stem(palavra)\n",
    "        stems.append(stem)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratar_verbos(palavras):    \n",
    "    lemmas = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for palavra in palavras:\n",
    "        lemma = lemmatizer.lemmatize(palavra, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567ae629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar(palavras):\n",
    "    palavras = remove_nao_ascii(palavras)\n",
    "    palavras = caixa_baixa(palavras)\n",
    "    palavras = remove_pontuacao(palavras)\n",
    "    palavras = troca_numero_palavra(palavras)\n",
    "    palavras = remove_stopwords(palavras)    \n",
    "    palavras = tratar_verbos(palavras)\n",
    "    #palavras = tratar_troncos(palavras)\n",
    "    return palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c08edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = MongoClient('mongodb://usrtcc:usrtcc@192.168.160.4:27017/?authSource=tcc&readPreference=primary&ssl=false')\n",
    "db = mc.tcc\n",
    "bolsaspalavras = db.bolsaspalavras\n",
    "frequenciatwitter = db.frequenciatwitter\n",
    "\n",
    "bolsaspalavras.drop()\n",
    "dicionario = pd.read_csv('usa.csv')\n",
    "dias = list(frequenciatwitter.distinct('data_coleta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d14be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dia in dias:\n",
    "    ini = time.time()\n",
    "    cursor = frequenciatwitter.find({'data_coleta': dia})\n",
    "    for posicao in cursor:\n",
    "        try:\n",
    "            id = posicao['_id']\n",
    "            frequencia = posicao['frequencia']\n",
    "            bolsa = posicao['tweet']\n",
    "            bolsa = re.sub(r\"http\\S+\", \"\", bolsa)\n",
    "            bolsa = remove_invalidas(bolsa)\n",
    "            bolsa = contractions.fix(bolsa)\n",
    "            bolsa = nltk.word_tokenize(bolsa)\n",
    "            bolsa = normalizar(bolsa)\n",
    "            bolsa = list(dict.fromkeys(bolsa))\n",
    "            normalizada = []\n",
    "            for palavra in bolsa:\n",
    "                existe = dicionario.loc[dicionario['palavra'] == palavra]\n",
    "                if not existe.empty:\n",
    "                    if (existe.size > 0) and (len(palavra) > 1) and (len(palavra) < 16):\n",
    "                        normalizada.append(palavra)\n",
    "            if normalizada != []:\n",
    "                caractere = ' '\n",
    "                ordenar = sorted(normalizada)\n",
    "                chave = caractere.join(ordenar)\n",
    "                documento = {\n",
    "                    '_id': id,\n",
    "                    'data_coleta': dia,\n",
    "                    'chave': chave,\n",
    "                    'frequencia': frequencia,\n",
    "                    'bolsa': ordenar\n",
    "                }\n",
    "                bolsaspalavras.insert_one(documento).inserted_id\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    fim = time.time()\n",
    "    print('--> ' + dia + ' - ' + str(fim-ini))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
