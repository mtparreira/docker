{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ac29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import inflect\n",
    "import unicodedata\n",
    "import contractions\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk        import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem   import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1f669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nao_ascii(palavras):\n",
    "    novas_palavras = []\n",
    "    for palavra in palavras:\n",
    "        nova_palavra = unicodedata.normalize('NFKD', palavra).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        novas_palavras.append(nova_palavra)\n",
    "    return novas_palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a796a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caixa_baixa(palavras):\n",
    "    novas_palavras = []\n",
    "    for palavra in palavras:\n",
    "        nova_palavra = palavra.lower()\n",
    "        novas_palavras.append(nova_palavra)\n",
    "    return novas_palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c18ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pontuacao(palavras):\n",
    "    novas_palavras = []\n",
    "    for palavra in palavras:\n",
    "        nova_palavra = re.sub(r'[^\\w\\s]', '', palavra)\n",
    "        if nova_palavra != '':\n",
    "            novas_palavras.append(nova_palavra)\n",
    "    return novas_palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d43004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def troca_numero_palavra(palavras):\n",
    "    novas_palavras = []\n",
    "    ie = inflect.engine()\n",
    "    for palavra in palavras:\n",
    "        if palavra.isdigit():\n",
    "            nova_palavra = ie.number_to_words(palavra)\n",
    "            novas_palavras.append(nova_palavra)\n",
    "        else:\n",
    "            novas_palavras.append(palavra)\n",
    "    return novas_palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c534a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(palavras):\n",
    "    novas_palavras = []\n",
    "    for palavra in palavras:\n",
    "        if palavra not in stopwords.words('english'):\n",
    "            novas_palavras.append(palavra)\n",
    "    return novas_palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e1de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratar_troncos(palavras):    \n",
    "    stems = []\n",
    "    stemmer = LancasterStemmer()\n",
    "    for palavra in palavras:\n",
    "        stem = stemmer.stem(palavra)\n",
    "        stems.append(stem)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratar_verbos(palavras):    \n",
    "    lemmas = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for palavra in palavras:\n",
    "        lemma = lemmatizer.lemmatize(palavra, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567ae629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar(palavras):\n",
    "    palavras = remove_nao_ascii(palavras)\n",
    "    palavras = caixa_baixa(palavras)\n",
    "    palavras = remove_pontuacao(palavras)\n",
    "    palavras = troca_numero_palavra(palavras)\n",
    "    palavras = remove_stopwords(palavras)    \n",
    "    palavras = tratar_verbos(palavras)\n",
    "    #palavras = tratar_troncos(palavras)\n",
    "    return palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "frase = \"Cole uma frase aqui em inglês\"\n",
    "lista_palavras = frase\n",
    "# Remove URL\n",
    "lista_palavras = re.sub(r\"http\\S+\", \"\", lista_palavras)\n",
    "# Remove contrações\n",
    "lista_palavras = contractions.fix(lista_palavras)\n",
    "# Tokenize\n",
    "lista_palavras = nltk.word_tokenize(lista_palavras)\n",
    "lista_token = lista_palavras\n",
    "# Normalização\n",
    "lista_palavras = normalizar(lista_palavras)\n",
    "# Retira duplicidades\n",
    "lista_palavras_unicas = list(dict.fromkeys(lista_palavras))\n",
    "# Resultado\n",
    "print(frase)\n",
    "print(lista_token)\n",
    "print(lista_palavras)\n",
    "print(lista_palavras_unicas)\n",
    "print(tratar_troncos(lista_palavras_unicas))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c08edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = MongoClient('mongodb://usrtcc:usrtcc@10.10.0.5:27017/?authSource=tcc&readPreference=primary&ssl=false')\n",
    "db = mc.tcc\n",
    "twitters = db.twitters\n",
    "bagofwords = db.bagofwords\n",
    "#bagofwords.drop()\n",
    "\n",
    "id = ''\n",
    "bolsa = ''\n",
    "documento = ''\n",
    "\n",
    "referencia = '2021-01-01'\n",
    "qtde = bagofwords.count();\n",
    "if qtde > 0:\n",
    "    ultimo_dia = bagofwords.find_one({},sort=[(\"data_coleta\", -1)])\n",
    "    referencia = ultimo_dia['data_coleta']\n",
    "\n",
    "dias = list(db.twitters.distinct('data_coleta'))\n",
    "for dia in dias:\n",
    "    if dia >= referencia:\n",
    "        bagofwords.remove ({'data_coleta': dia})\n",
    "        horas = ['00','01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23']\n",
    "        for hora in horas:\n",
    "            hora_ini = hora + ':00:00'\n",
    "            hora_fim = hora + ':59:59'        \n",
    "            cursor = twitters.find({'data_coleta': dia,'hora_coleta': {'$gte': hora_ini, '$lt': hora_fim} })\n",
    "            for documento in cursor:       \n",
    "                id = documento['_id']\n",
    "                bolsa = documento['tweet']\n",
    "                bolsa = re.sub(r\"http\\S+\", \"\", bolsa)\n",
    "                bolsa = contractions.fix(bolsa)\n",
    "                bolsa = nltk.word_tokenize(bolsa)\n",
    "                bolsa = normalizar(bolsa)\n",
    "                bolsa = list(dict.fromkeys(bolsa))\n",
    "                documento = {\n",
    "                    '_id': id,\n",
    "                    'data_coleta': dia,\n",
    "                    'bolsa': bolsa\n",
    "                }\n",
    "                db.bagofwords.insert_one(documento).inserted_id\n",
    "            print(dia + ' - ' + hora_ini + ' - ' + hora_fim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
